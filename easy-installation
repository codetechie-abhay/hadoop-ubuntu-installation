To simplify the installation process, you can create a shell script that automates the steps. 
Here is a script you can use:

1. Create a new file named `install_hadoop.sh` in your home directory:

```sh
nano ~/install_hadoop.sh
```

2. Copy and paste the following script into the `install_hadoop.sh` file:

```sh
#!/bin/bash

# Update and upgrade the system
sudo apt update && sudo apt upgrade -y

# Install OpenSSH server and client
sudo apt install openssh-server openssh-client -y

# Generate SSH keys
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
chmod 700 ~/.ssh

# Verify SSH setup
ssh localhost

# Install Java
sudo apt install openjdk-11-jdk -y

# Download and extract Hadoop
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar xzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /usr/local/hadoop

# Configure environment variables
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> ~/.bashrc
echo "export HADOOP_HOME=/usr/local/hadoop" >> ~/.bashrc
echo "export HADOOP_INSTALL=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_MAPRED_HOME=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_COMMON_HOME=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_HDFS_HOME=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_YARN_HOME=\$HADOOP_HOME" >> ~/.bashrc
echo "export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_HOME/lib/native" >> ~/.bashrc
echo "export PATH=\$PATH:\$HADOOP_HOME/sbin:\$HADOOP_HOME/bin" >> ~/.bashrc
echo "export HADOOP_OPTS=\"-Djava.library.path=\$HADOOP_HOME/lib/native\"" >> ~/.bashrc
echo "export PDSH_RCMD_TYPE=ssh" >> ~/.bashrc
source ~/.bashrc

# Configure Hadoop files
HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop

# core-site.xml
cat <<EOL > \$HADOOP_CONF_DIR/core-site.xml
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
EOL

# hdfs-site.xml
cat <<EOL > \$HADOOP_CONF_DIR/hdfs-site.xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/$USER/hadoop2-dir/namenode-dir</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/$USER/hadoop2-dir/datanode-dir</value>
    </property>
</configuration>
EOL

# mapred-site.xml
cat <<EOL > \$HADOOP_CONF_DIR/mapred-site.xml
<configuration>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop/etc/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop/etc/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/hadoop/etc/hadoop</value>
    </property>
</configuration>
EOL

# yarn-site.xml
cat <<EOL > \$HADOOP_CONF_DIR/yarn-site.xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <description>The hostname of the RM.</description>
        <name>yarn.resourcemanager.hostname</name>
        <value>localhost</value>
    </property>
    <property>
        <description>The address of the applications manager interface in the RM.</description>
        <name>yarn.resourcemanager.address</name>
        <value>\${yarn.resourcemanager.hostname}:8032</value>
    </property>
</configuration>
EOL

# hadoop-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> \$HADOOP_CONF_DIR/hadoop-env.sh

# mapred-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> \$HADOOP_CONF_DIR/mapred-env.sh

# yarn-env.sh
echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> \$HADOOP_CONF_DIR/yarn-env.sh

# Format HDFS
cd /usr/local/hadoop
hdfs namenode -format

# Start Hadoop services
start-all.sh

# Print completion message
echo "ðŸŽ‰ Hadoop setup complete successfully! ðŸŽ‰"
```

3. Save the file and exit the editor (`Ctrl+X`, then `Y`, and `Enter`).

4. Make the script executable:

```sh
chmod +x ~/install_hadoop.sh
```

5. Run the script:

```sh
~/install_hadoop.sh
```
